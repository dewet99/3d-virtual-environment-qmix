# --- QMIX specific parameters ---
# --- Agent Parameters ---
agent: "icm_agent" # Default rnn agent
obs_agent_id: True
rnn_hidden_dim: 64 # Size of hidden state for default rnn agent
useNoisy: False # Whether noisy NN should be used
num_random_steps: 0 #The number of episodes where a random policy should be used, to start populating replay and stabilise ICM

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 500000
t_max: 6000 #number of timesteps to train for before interrupting

runner: "ray_runner"
mac: "custom_mac" # Basic controller
learner: "ray_learner"
use_ray: True

worker_parameter_sync_frequency: 1 # How many parameter updates should occur before the workers sync. 

# --- REPLAY Parameters ---

prioritized_buffer_alpha: 0.6
per_eta: 0.9


# --- RL hyperparameters ---
batch_size: 32 # number of episodes used to backprop and update networks
recurrent_sequence_length: 96 # how many transitions should be sampled from each episode in the batch
random_update: True # whether a subset sequence of transitions should be sampled from each episode
buffer_size: 1440
lr: 0.0001 # Learning rate for agents
critic_lr: 0.0001 # Learning rate for critics
optim_alpha: 0.99 # RMSProp alpha
optim_eps: 0.00001 # RMSProp epsilon
grad_norm_clip: 20 # Reduce magnitude of gradients above this L2 norm



#--- R2D2 Stuff ---
# use_burnin implies that we will store hidden states

burn_in_step_count: 32 #number of steps to burn hidden state in
value_fn_rescaling: True

n_step: 8

#Ablation
num_executors: 8
use_burnin: True
n_step_return: True
standardise_rewards: True
use_per: True # use prioritized experience replay

#Transfer_learning:
use_transfer: False
models_2_transfer_path: "./results/full_space_succeed_transfer/models/15000"


# --- Stuff ---

reward_clipping: True
reward_clip_max: 5
reward_clip_min: -5
log_histograms: False


#--- Curiosity ---
curiosity: False
normalise_obs: False
use_curiosity_reward: True
normalising_episodes: 10
eta: 1.0 # weighing factor for individual intrinsic rewards
max_instrinsic_reward_sum: 20 # The intrinsic reward is normalised so that the sum of all intrinsic rewards in an episode is equal to this value
beta: 0.2
lamda: 0.1
pred_lr_scale: 1.0
min_icm_reward_gain: 0.000001 #minimum value that the icm reward is multiplied by
icm_weight: 0.003


# update the target network every {} episodes
target_update_interval: 100
num_training_steps_per_xp: 2

# use the Q_Learner to train
agent_output_type: "q"
double_q: True
mixer: "qmix"
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

# --- Environment Specific Options ---
time_scale: 5
executable_path: "./environment_executables/Ablation_Reduced_Act_Space/Ablation_Reduced_Act_Space.x86_64"
num_agents: 2
episode_limit: 500 # max number of steps in an episode
grayscale: False

name: "vis_qmix"

# --- Encoder Options ---
encoder: "nature"
encoder_output_size: 256
load_pretrained_model: False

# --- Test ---
save_obs_for_debug: False
test_executable_path: "./environment_executables/experiment_5_visible/experiment_5_visible.x86_64"
test_models_path: "./results/experiment_5_transfer/models/40000"
num_test_episodes: 400
test_timescale: 1


# --- Logging options ---
use_tensorboard: True # Log results to tensorboard
save_models: True # Save the models to disk
save_models_interval: 500 # Save models after this many trainer steps
load_models_from: "" # Load a checkpoint from this path
set_results_dir_to: ""
tb_directory: ""
encoder_logs_dir: ""
local_results_path: "results" # Path for local results
log_every: 20 #log every time this number of trainer steps has elapsed



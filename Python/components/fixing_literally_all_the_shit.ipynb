{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributing QMIX with a visual feature extractor over multiple CPU cores\n",
    "## Table of Contents:\n",
    "    - Load a config file\n",
    "    - run environment from config file\n",
    "    - spawn multiple environments\n",
    "    - spawn replay server\n",
    "    - spawn parameter server\n",
    "    - spawn learner process\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.read_config import merge_yaml_files, merge_dicts\n",
    "\n",
    "file1 = \"./config/default.yaml\"\n",
    "file2 = \"./config/visual_qmix.yaml\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running environment given a config file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import (EngineConfigurationChannel,)\n",
    "from wrappers.UnityParallelEnvWrapper_Torch import UnityWrapper\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "import pdb\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import gc\n",
    "from functools import partial\n",
    "from utils.unity_utils import get_worker_id\n",
    "import torch\n",
    "from controllers.custom_controller import CustomMAC\n",
    "from components.replay_buffer import EpisodeBatch\n",
    "from utils.utils import OneHot, RunningMeanStdTorch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from components.replay_buffer import Remote_ReplayBuffer\n",
    "from components.parameter_server import ParameterServer\n",
    "\n",
    "# @ray.remote(num_cpus = 1, num_gpus = 0.01)\n",
    "class Executor:\n",
    "    def __init__(self,config, worker_id):\n",
    "        super().__init__()\n",
    "        # Set config items\n",
    "        self.time_scale = config[\"time_scale\"]\n",
    "        self.env_path = config[\"executable_path\"]\n",
    "        self.episode_limit = config[\"episode_limit\"]\n",
    "        self.config = config\n",
    "        self.batch_size = config[\"batch_size_run\"]\n",
    "\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.worker_id = worker_id\n",
    "\n",
    "        # Set class variables\n",
    "        config_channel = EngineConfigurationChannel()\n",
    "        config_channel.set_configuration_parameters(time_scale=self.time_scale)\n",
    "\n",
    "        try:\n",
    "            self.env.close()\n",
    "            self.unity_env.close()\n",
    "        except:\n",
    "            print(\"No envs open\")\n",
    "\n",
    "        unity_env = UnityEnvironment(file_name=self.env_path, worker_id=get_worker_id(), seed=np.int32(0), side_channels=[config_channel])\n",
    "        # unity_env = UnityEnvironment(file_name='./unity/envs/Discrete_NoCur/Discrete_NoCur.x86_64', worker_id=get_worker_id())\n",
    "        unity_env.reset()\n",
    "\n",
    "        self.env = UnityWrapper(unity_env, config_channel, episode_limit=self.episode_limit)\n",
    "        self.env.reset()\n",
    "\n",
    "        self.get_env_info()\n",
    "        self.setup()\n",
    "        self.setup_logger()\n",
    "\n",
    "        if self.config[\"curiosity\"]:\n",
    "            self.reward_rms = RunningMeanStdTorch()\n",
    "\n",
    "\n",
    "    def collect_experience(self):\n",
    "        self.reset()\n",
    "        episode_start = time.time()\n",
    "        # global_steps = ray.get(self.parameter_server.return_environment_steps.remote())\n",
    "        global_steps = 123\n",
    "        terminated = False\n",
    "        episode_return = 0\n",
    "        \n",
    "\n",
    "        self.mac.init_hidden(batch_size=self.batch_size)\n",
    "        raw_observations = 0\n",
    "\n",
    "        reward_episode = []\n",
    "        icm_reward = None\n",
    "\n",
    "        while not terminated:\n",
    "\n",
    "            raw_observations = self.env._get_observations()       \n",
    "\n",
    "            # state is determined from raw obs after feature extraction\n",
    "            # normalise the obs before you save them to the replay buffer\n",
    "            # if self.config[\"contains_state\"]:\n",
    "            state = self.env._get_global_state_variables()\n",
    "\n",
    "            pre_transition_data = {\n",
    "                \"state\": state,\n",
    "                \"avail_actions\": self.env.get_avail_actions(),\n",
    "                \"obs\": raw_observations\n",
    "            }\n",
    "            # else:\n",
    "            #     pre_transition_data = {\n",
    "            #         \"avail_actions\": self.env.get_avail_actions(),\n",
    "            #         \"obs\": raw_observations\n",
    "            #     }\n",
    "\n",
    "            self.batch.update(pre_transition_data, ts=self.t)\n",
    "\n",
    "            # Pass the entire batch of experiences up till now to the \n",
    "            # Receive the actions for each agent at this timestep in a batch of size 1\n",
    "            # This will change depending on whether I'm using a feature extraction network or not\n",
    "            actions = self.mac.select_actions(self.batch, t_ep=self.t, t_env=global_steps, test_mode=False)\n",
    "\n",
    "\n",
    "            reward, terminated, env_info = self.env.step(actions[0])\n",
    "            reward_episode.append(reward)\n",
    "\n",
    "\n",
    "            episode_return += reward\n",
    "\n",
    "\n",
    "            post_transition_data = {\n",
    "                \"actions\": actions,\n",
    "                \"reward\": [(reward,)],\n",
    "                \"terminated\": [(terminated != env_info.get(\"episode_limit\", False),)],\n",
    "                # terminated above says whether the agent terminated because they reached the end\n",
    "                # of the episode\n",
    "            }\n",
    "\n",
    "            self.batch.update(post_transition_data, ts=self.t)\n",
    "\n",
    "            self.t += 1\n",
    "\n",
    "        raw_observations = self.env._get_observations()\n",
    "\n",
    "        last_data = {\n",
    "                \"state\": self.env._get_global_state_variables(),\n",
    "                \"avail_actions\": self.env.get_avail_actions(),\n",
    "                \"obs\": raw_observations\n",
    "            }\n",
    "        \n",
    "        self.batch.update(last_data, ts=self.t)\n",
    "\n",
    "        actions = self.mac.select_actions(self.batch, t_ep=self.t, t_env=global_steps, test_mode=False)\n",
    "\n",
    "        self.batch.update({\"actions\": actions}, ts=self.t)\n",
    "\n",
    "        if self.config[\"curiosity\"]:\n",
    "            icm_reward = self.curiosity()\n",
    "            icm_reward = np.sum(icm_reward, axis=-1)\n",
    "        \n",
    "        try:\n",
    "            # Parameter server keeps track of global steps and episodes\n",
    "            # Add the number of steps in this executor's episode to the global count\n",
    "            self.parameter_server.add_environment_steps.remote(self.t)\n",
    "            \n",
    "            # Increment global episode count\n",
    "            self.parameter_server.increment_total_episode_count.remote()\n",
    "\n",
    "            # Accumulate mean episode reward and episode duration\n",
    "            self.parameter_server.accumulate_stats.remote(sum(reward_episode), time.time() - episode_start, icm_reward)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return self.batch\n",
    "    \n",
    "    def run(self, remote_buffer, parameter_server):\n",
    "        self.parameter_server = parameter_server\n",
    "        self.remote_buffer = remote_buffer\n",
    "\n",
    "        while True:\n",
    "            # TODO : Sample from parameter server every few episodes to obtain up to date parameters\n",
    "            # Sample every 10 parameter updates:\n",
    "            if ray.get(self.parameter_server.get_parameter_update_steps.remote()) % 10 == 0:\n",
    "                self.sync_with_parameter_server()\n",
    "\n",
    "\n",
    "            episode_batch = self.collect_experience()\n",
    "\n",
    "            # Trying to use shared memory\n",
    "            episode_batch_reference = ray.put(episode_batch)\n",
    "\n",
    "            self.remote_buffer.insert_episode_batch.remote(episode_batch_reference)\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.batch = self.new_batch()\n",
    "        self.env.reset()\n",
    "        self.t = 0\n",
    "        self.mac.agent.fc2.eval()\n",
    "        self.mac.agent.feature_extractor.eval()\n",
    "\n",
    "    def curiosity(self):\n",
    "        T = self.batch.max_t_filled()\n",
    "        observation = self.batch[\"obs\"][:, :T]\n",
    "        B = observation.shape[0]\n",
    "        N = observation.shape[2]\n",
    "\n",
    "        shaped_obs = observation.reshape(-1, self.mac.agent.feature_extractor.height,\n",
    "                self.mac.agent.feature_extractor.width,\n",
    "                self.mac.agent.feature_extractor.initial_channels).cuda()\n",
    "\n",
    "        reduced_obs = self.mac.agent.feature_extractor(shaped_obs)\n",
    "        reduced_obs = reduced_obs.reshape(B, T, N, -1).cuda()\n",
    "        next_obs, obs = self.mac.agent._build_batch_inputs(reduced_obs, self.batch)\n",
    "\n",
    "        real_next_obs, pred_next_obs, pred_action = self.mac.agent.icm([obs[:, :T], next_obs[:, :T], self.batch[\"actions_onehot\"][:, :T-1]])\n",
    "\n",
    "        intrinsic_reward = self.config[\"eta\"] * F.mse_loss(\n",
    "        real_next_obs, pred_next_obs, reduction='none').mean(-1).data\n",
    "\n",
    "        intrinsic_reward = torch.mean(intrinsic_reward, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        self.reward_rms.update(intrinsic_reward)\n",
    "        # self.reward_rms.update_from_moments(mean, std**2, count)\n",
    "        intrinsic_reward_scaled = (intrinsic_reward-self.reward_rms.mean)/torch.sqrt(self.reward_rms.var)\n",
    "        # print(f\"Intrinsic reward before norm: {intrinsic_reward}\")\n",
    "        # print(f\"Intrinsic reward after norm: {intrinsic_reward_scaled}\")\n",
    "        self.batch.update({\"icm_reward\": intrinsic_reward_scaled}, ts = slice(0,self.t))\n",
    "\n",
    "        return np.sum(intrinsic_reward_scaled.cpu().numpy(), axis = -1)\n",
    "\n",
    "    \n",
    "    def setup(self):\n",
    "        scheme, groups, preprocess = self.generate_scheme()\n",
    "        self.mac = CustomMAC(self.config)\n",
    "\n",
    "        self.new_batch = partial(EpisodeBatch, scheme, groups, self.config[\"batch_size_run\"], self.config[\"episode_limit\"]+1, preprocess = preprocess, device = \"cpu\")\n",
    "\n",
    "\n",
    "    def get_env_info(self):\n",
    "        self.config[\"obs_shape\"] = self.env.obs_shape\n",
    "        self.env_info = self.env.get_init_env_info()\n",
    "        self.config[\"n_actions\"] = self.env_info[\"n_actions\"]\n",
    "\n",
    "    def setup_logger(self):\n",
    "        #TODO\n",
    "        pass\n",
    "\n",
    "    def close_env(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def generate_scheme(self):\n",
    "        self.config[\"state_shape\"] = self.env_info[\"state_shape\"]\n",
    "\n",
    "        scheme = {\n",
    "            \"state\": {\"vshape\": self.env_info[\"state_shape\"]},\n",
    "            \"obs\": {\"vshape\": self.env_info[\"obs_shape\"], \"group\": \"agents\", \"dtype\": torch.float32},\n",
    "            \"actions\": {\"vshape\": (1,), \"group\": \"agents\", \"dtype\": torch.long},\n",
    "            \"avail_actions\": {\"vshape\": (self.env_info[\"n_actions\"],), \"group\": \"agents\", \"dtype\": torch.int},\n",
    "            \"reward\": {\"vshape\": (1,)},\n",
    "            \"terminated\": {\"vshape\": (1,), \"dtype\": torch.uint8},\n",
    "            }\n",
    "        \n",
    "        if self.config[\"curiosity\"]:\n",
    "            icm_reward = {\"icm_reward\": {\"vshape\": (1,)},}\n",
    "            scheme.update(icm_reward)\n",
    "\n",
    "        if self.config[\"useNoisy\"]:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        groups = {\n",
    "        \"agents\": self.config[\"num_agents\"]\n",
    "        }\n",
    "\n",
    "        preprocess = {\n",
    "        \"actions\": (\"actions_onehot\", [OneHot(out_dim=self.config[\"n_actions\"])])\n",
    "        }\n",
    "\n",
    "        return scheme, groups, preprocess\n",
    "    \n",
    "    def retrieve_updated_config(self):\n",
    "        return self.config\n",
    "    \n",
    "    def sync_with_parameter_server(self):\n",
    "        # receive the stored parameters from the server using ray.get()\n",
    "        new_params = ray.get(self.parameter_server.return_params.remote())\n",
    "        \n",
    "        # copy the received neural network weights to its own\n",
    "        # since the weights are saved as numpy arrays in the server,\n",
    "        # we have to convert them into pytorch tensors. \n",
    "        for param, new_param in zip(self.mac.parameters(), new_params):\n",
    "            new_param = torch.Tensor(new_param).to(self.device)\n",
    "            param.data.copy_(new_param)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the replay buffer given config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from components.replay_buffer import Remote_ReplayBuffer, generate_replay_scheme\n",
    "# from components.parameter_server import ParameterServer\n",
    "\n",
    "\n",
    "# config = merge_yaml_files(file1, file2)\n",
    "\n",
    "# workers = [Executor.remote(config, i) for i in range (config[\"num_executors\"])]\n",
    "# config_ref = workers[0].retrieve_updated_config.remote()\n",
    "# config = ray.get(config_ref)\n",
    "\n",
    "# scheme, groups, preprocess = generate_replay_scheme(config)\n",
    "\n",
    "# remote_buffer = Remote_ReplayBuffer.remote(scheme, groups, config[\"buffer_size\"], config[\"episode_limit\"]+1, preprocess=preprocess, device=\"cpu\")\n",
    "# parameter_server = ParameterServer.remote()\n",
    "\n",
    "# ray.wait([worker.run.remote(parameter_server, remote_buffer) for worker in workers])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Learner for Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from models.qmix import QMixer\n",
    "from utils.utils import RunningMeanStdTorch\n",
    "from torch.optim import Adam\n",
    "from copy import deepcopy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "# @ray.remote(num_gpus = 0.96, num_cpus=3)\n",
    "class Learner(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = \"cuda:0\"\n",
    "        self.global_training_steps = config[\"t_max\"]\n",
    "        self.mac = CustomMAC(self.config)\n",
    "\n",
    "        self.trainable_parameters = nn.ParameterList(self.mac.parameters())\n",
    "\n",
    "        self.beta = self.config[\"beta\"]\n",
    "\n",
    "\n",
    "        # Parameter server stuff\n",
    "        self.parameter_server_list = list (self.mac.agent.state_dict())\n",
    "\n",
    "\n",
    "        # Setup Mixer\n",
    "        self.mixer = QMixer(config)\n",
    "        self.trainable_parameters+= nn.ParameterList(self.mixer.parameters())\n",
    "        \n",
    "\n",
    "        # Reward Standardisation\n",
    "        if self.config[\"standardise_rewards\"]:\n",
    "            self.reward_rms = RunningMeanStdTorch(shape=(1,), device=\"cuda:0\")\n",
    "\n",
    "        # Optimiser\n",
    "        self.optimiser = Adam(params=self.trainable_parameters, lr=self.config[\"lr\"], eps = self.config[\"optim_eps\"])\n",
    "\n",
    "        # Loss functions\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Target Networks:\n",
    "        # a little wasteful to deepcopy (e.g. duplicates action selector), but should work for any MAC (this is what the pymarl dudes said, bless their innocent hearts)\n",
    "        self.target_mac = deepcopy(self.mac)\n",
    "        self.target_mixer = deepcopy(self.mixer)\n",
    "\n",
    "        self.previous_target_update_episode = 0\n",
    "\n",
    "        self.trainer_steps = 0\n",
    "\n",
    "        self.cuda()\n",
    "\n",
    "        self.debug=0\n",
    "\n",
    "        # Logger Stuff\n",
    "        self.training_start_time = time.time()\n",
    "        self.setup_writer()\n",
    "        self.time_info = {}\n",
    "        self.log_stats_dict = {}\n",
    "\n",
    "    def train(self, log_this_step = False):\n",
    "        print(\"Getting Batch\")\n",
    "        if self.config[\"use_per\"]:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            # episode_sample_reference = self.remote_buffer.sample.remote(self.config[\"batch_size\"])\n",
    "            episode_sample_reference = self.remote_buffer.sample(self.config[\"batch_size\"])\n",
    "            # episode_sample = ray.get(episode_sample_reference)\n",
    "        print(\"Got episode sample reference\")\n",
    "\n",
    "        self.debug+=1\n",
    "        print(f\"Sizeof ep ref: {sys.getsizeof(episode_sample_reference)}\")\n",
    "        # max_ep_t_reference = ray.get(episode_sample_reference.max_t_filled.remote())\n",
    "        max_ep_t_reference = episode_sample_reference.max_t_filled()\n",
    "        print(f\"Sizeof max_ep_t_reference: {sys.getsizeof(max_ep_t_reference)}\")\n",
    "        print(\"Got max_ep_t_reference\")\n",
    "        # max_ep_t = episode_sample.max_t_filled()\n",
    "\n",
    "        # max_ep_t = ray.get(max_ep_t_reference)\n",
    "        max_ep_t = max_ep_t_reference\n",
    "        print(\"Got max_ep_t\")\n",
    "        print(\"Slicing batch\")\n",
    "        if not self.config[\"random_update\"]:\n",
    "            # episode_sample = episode_sample[:, :max_ep_t]\n",
    "            episode_sample_reference = episode_sample_reference[:, :max_ep_t]\n",
    "        else:\n",
    "            if max_ep_t>self.config[\"recurrent_sequence_length\"]:\n",
    "                start_idx = np.random.randint(0, max_ep_t-self.config[\"recurrent_sequence_length\"]+1)\n",
    "                # episode_sample = episode_sample[:, start_idx[0]:start_idx[0]+self.config[\"recurrent_sequence_length\"]]\n",
    "                episode_sample_reference = episode_sample_reference[:, start_idx[0]:start_idx[0]+self.config[\"recurrent_sequence_length\"]]\n",
    "            else:\n",
    "                # episode_sample = episode_sample[:, :max_ep_t]\n",
    "                episode_sample_reference = episode_sample_reference[:, :max_ep_t]\n",
    "        print(\"Sliced batch\")\n",
    "        if self.config[\"use_per\"]:\n",
    "            # masked_td_error, mask = self.subtrain(batch, t_env, episode_num)\n",
    "            # res = th.sum(masked_td_error, dim=(1,2)) / th.sum(mask, dim = (1,2))\n",
    "            # res = res.cpu().detach().numpy()\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            print(\"Run Training LOOP\")\n",
    "            # log_dict = self.training_loop(episode_sample, log_this_step)\n",
    "            # log_dict = self.training_loop(ray.put(episode_sample_reference), log_this_step)\n",
    "            log_dict = self.training_loop(episode_sample_reference, num_global_episodes=123, log_this_step = log_this_step)\n",
    "            print(\"Training LOOP Done\")\n",
    "        return log_dict\n",
    "\n",
    "    def run(self, remote_buffer, parameter_server):\n",
    "        self.remote_buffer = remote_buffer\n",
    "        self.parameter_server = parameter_server\n",
    "        debug_steps = 0\n",
    "        while not ray.get(self.remote_buffer.can_sample.remote(self.config[\"batch_size\"])):\n",
    "            debug_steps+=1\n",
    "            if debug_steps%10000 == 0:\n",
    "                print(f\"Waited {debug_steps} steps\")\n",
    "                print(ray.get(self.remote_buffer.__repr__.remote()))\n",
    "            continue\n",
    "\n",
    "        print(\"READY TO START TRAINING!!\")\n",
    "\n",
    "        while True:\n",
    "            # Log time taken per training step:\n",
    "            current_training_step_start_time = time.time()\n",
    "            \n",
    "            if self.trainer_steps%self.config[\"log_every\"] == 0:\n",
    "                log_this_step = True\n",
    "            else:\n",
    "                log_this_step = False\n",
    "\n",
    "            print(\"Starting training step\")\n",
    "            log_dict = self.train(log_this_step)\n",
    "            print(\"Training step done\")\n",
    "            self.trainer_steps += 1\n",
    "            self.update_parameter_server()\n",
    "            \n",
    "            training_took = time.time() - current_training_step_start_time\n",
    "            self.store_time_stats(\"Mean_training_loop_time\", training_took)\n",
    "\n",
    "            if log_this_step:\n",
    "                self.log_stats(log_dict)\n",
    "            \n",
    "\n",
    "        \n",
    "    def setup_writer(self):\n",
    "        self.writer = SummaryWriter(log_dir=\"results/\" + datetime.datetime.now().strftime(\"%d_%m_%H_%M\") + \"/tb_logs\")\n",
    "\n",
    "    def log_stats(self, stats_to_log:dict):\n",
    "        global_environment_steps = ray.get(self.parameter_server.return_environment_steps.remote())\n",
    "        # Stats obtained from the trainer:\n",
    "        self.writer.add_scalars(\"Training_Stats\", stats_to_log, global_environment_steps)\n",
    "        \n",
    "        # Total elapsed training time\n",
    "        self.writer.add_scalar(\"Time_Stats\", time.time() - self.training_start_time, global_environment_steps)\n",
    "\n",
    "        # Trainer time info\n",
    "        self.writer.add_scalar(\"Time_Stats\", self.time_info[\"Mean_training_loop_time\"]/self.time_info[\"number_log_steps\"], global_environment_steps)\n",
    "\n",
    "        # Log Executors' rewards\n",
    "        mean_extrinsic_reward, mean_icm_reward, mean_ep_duration = ray.get(self.parameter_server.get_accumulated_stats.remote())\n",
    "        self.writer.add_scalar(\"Reward_Stats\", mean_extrinsic_reward, global_environment_steps)\n",
    "        if mean_icm_reward is not None:\n",
    "            self.writer.add_scalar(\"Reward_Stats\", mean_icm_reward, global_environment_steps)\n",
    "        self.writer.add_scalar(\"Time_Stats\", mean_ep_duration, global_environment_steps)\n",
    "\n",
    "    def periodically_print(self):\n",
    "        if time.time() - self.training_start_time > 20:\n",
    "            ray.get(self.remote_buffer.__repr__.remote())\n",
    "\n",
    "    def store_time_stats(self, key, value):\n",
    "        self.time_info[key] += value\n",
    "        self.time_info[\"number_log_steps\"] +=1\n",
    "\n",
    "    def reset_stats():\n",
    "        pass\n",
    "           \n",
    "\n",
    "    def training_loop(self, batch: EpisodeBatch, num_global_episodes, log_this_step):\n",
    "        print(\"Setting encoder to train\")\n",
    "        self.mac.agent.feature_extractor.train()\n",
    "\n",
    "        print(\"Cudaing all parts\")\n",
    "        # Get the relevant quantities\n",
    "        rewards = batch[\"reward\"][:, :-1].cuda()\n",
    "        actions = batch[\"actions\"][:, :-1].cuda()\n",
    "        actions_onehot = batch[\"actions_onehot\"][:,:-1].cuda()\n",
    "        terminated = batch[\"terminated\"][:, :-1].float().cuda()\n",
    "        mask = batch[\"filled\"][:, :-1].float().cuda()\n",
    "        mask[:, 1:] = mask[:, 1:] * (1 - terminated[:, :-1]).cuda()\n",
    "        avail_actions = batch[\"avail_actions\"].cuda()\n",
    "        print(\"Cudad all parts\")\n",
    "        if self.config[\"curiosity\"]:\n",
    "            icm_reward = batch[\"icm_reward\"][:, :-1].cuda()\n",
    "\n",
    "        if self.config[\"standardise_rewards\"]:\n",
    "            self.reward_rms.update(rewards)\n",
    "            rewards_normed = (rewards-self.reward_rms.mean)/torch.sqrt(self.reward_rms.var)\n",
    "\n",
    "\n",
    "        # Feature Extraction\n",
    "        # Gotta generate the state from the flattened observations still\n",
    "        # We get flat obs in shape: (B, T, N, 128)\n",
    "        # We want tp convert this into a tensor of shape (B,T,256)\n",
    "        B = batch[\"obs\"].shape[0]\n",
    "        T = batch[\"obs\"].shape[1]\n",
    "        N = batch[\"obs\"].shape[2]\n",
    "\n",
    "        observation = batch[\"obs\"]\n",
    "        print(\"Reshape and cuda obs\")\n",
    "        shaped_obs = observation.reshape(-1, self.mac.agent.feature_extractor.height,\n",
    "                self.mac.agent.feature_extractor.width,\n",
    "                self.mac.agent.feature_extractor.initial_channels).cuda()\n",
    "\n",
    "        reduced_obs = self.mac.agent.feature_extractor(shaped_obs)\n",
    "        # shaped_obs = shaped_obs.cpu()\n",
    "        reduced_obs = reduced_obs.reshape(B, T, N, -1).cuda()\n",
    "        print(\"Extracted feats and cudad\")\n",
    "        # Also calculate target reduced obs, for use with the target networks\n",
    "        target_reduced_obs = self.target_mac.agent.feature_extractor(shaped_obs)\n",
    "        target_reduced_obs = target_reduced_obs.reshape(B,T,N,-1).cuda()\n",
    "        print(\"Extracted target feats and cudad\")\n",
    "                   \n",
    "        # self.mac.agent.fc2.train()\n",
    "        # self.target_mac.agent.fc2.train()\n",
    "\n",
    "        # list_of_noises = [batch[\"noise_0_weight\"], batch[\"noise_0_bias\"], batch[\"noise_1_weight\"], batch[\"noise_1_bias\"]]\n",
    "        \n",
    "        # We have to pass the observations through the feature extractor before using them to calculate\n",
    "        # the agent outputs\n",
    "        # Calculate estimated Q-Values\n",
    "        mac_out = []\n",
    "        self.mac.init_hidden(batch.batch_size)\n",
    "        for t in range(batch.max_seq_length):\n",
    "            agent_outs = self.mac.forward(batch, t=t, training=True, batch_reduced_obs=reduced_obs)\n",
    "            mac_out.append(agent_outs) \n",
    "\n",
    "        mac_out = torch.stack(mac_out, dim=1)  # Concat over time\n",
    "\n",
    "        # Pick the Q-Values for the actions taken by each agent\n",
    "        chosen_action_qvals = torch.gather(mac_out[:, :-1], dim=3, index=actions).squeeze(3)  # Remove the last dim\n",
    "\n",
    "        # Calculate the Q-Values necessary for the target\n",
    "        target_mac_out = []\n",
    "        self.target_mac.init_hidden(batch.batch_size)\n",
    "        for t in range(batch.max_seq_length):\n",
    "            target_agent_outs = self.target_mac.forward(batch, t=t, training=True, batch_reduced_obs=target_reduced_obs)\n",
    "            target_mac_out.append(target_agent_outs)\n",
    "\n",
    "        # if self.feature_extractor:\n",
    "        #     # Create the state for mixing here, then delete the batch\n",
    "        #     del batch\n",
    "\n",
    "        # We don't need the first timesteps Q-Value estimate for calculating targets\n",
    "        target_mac_out = torch.stack(target_mac_out[1:], dim=1)  # Concat across time\n",
    "        # Mask out unavailable actions\n",
    "        target_mac_out[avail_actions[:, 1:] == 0] = -9999999\n",
    "\n",
    "        # Use target_meac_out as above for M-DQN. If using M-DQN, the modified target_max_q-values should be u\n",
    "\n",
    "        # Max over target Q-Values\n",
    "        if self.config[\"double_q\"]:\n",
    "            # Get actions that maximise live Q (for double q-learning)\n",
    "            mac_out_detach = mac_out.clone().detach()\n",
    "            mac_out_detach[avail_actions == 0] = -9999999\n",
    "            cur_max_actions = mac_out_detach[:, 1:].max(dim=3, keepdim=True)[1]\n",
    "            target_max_qvals = torch.gather(target_mac_out, 3, cur_max_actions).squeeze(3)\n",
    "        else:\n",
    "            target_max_qvals = target_mac_out.max(dim=3)[0]\n",
    "\n",
    "\n",
    "        # Mix\n",
    "        if self.mixer is not None:\n",
    "            # CURIOSITY\n",
    "            if self.config[\"curiosity\"]:\n",
    "                next_obs, obs = self.mac.agent._build_batch_inputs(reduced_obs, batch)\n",
    "\n",
    "                # Curiosity Loss:\n",
    "                real_next_obs, pred_next_obs, pred_action = self.mac.agent.icm([obs, next_obs, batch[\"actions_onehot\"][:, :-1]])\n",
    "\n",
    "                # Only use ICM while the agent is exploring\n",
    "                icm_reward_modded = icm_reward*self.config[\"icm_weight\"]\n",
    "\n",
    "                rewards_total = rewards_normed + icm_reward_modded\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                # inverse_loss = self.bce(pred_action, actions_onehot)\n",
    "                inverse_loss = self.ce(pred_action.contiguous().view(-1, self.config[\"n_actions\"]), actions_onehot.contiguous().view(-1,self.config[\"n_actions\"]))\n",
    "                \n",
    "                forward_loss = self.mse(\n",
    "                    pred_next_obs, real_next_obs.detach()\n",
    "                )\n",
    "            else:\n",
    "                # rewards = self.sum_norm(rewards, 20)\n",
    "                rewards_total = rewards_normed\n",
    "\n",
    "            # rewards_total = rewards\n",
    "            state = reduced_obs.view(B,T,-1)\n",
    "            target_state = target_reduced_obs.view(B,T,-1)\n",
    "            # Norms to 0 mean and 1 std\n",
    "            # Test without this to see if it influences anything\n",
    "            # state = z_score_norm(state)\n",
    "            \n",
    "            # state now contains the previous actions for all agents\n",
    "\n",
    "            # Also include the available actions at each timestep in the state:\n",
    "            state = torch.concat([state,avail_actions.reshape([B,T,-1])], dim=-1).cuda()\n",
    "            target_state = torch.concat([target_state,avail_actions.reshape([B,T,-1])], dim=-1).cuda()\n",
    "\n",
    "            # if self.config[\"contains_state\"]:\n",
    "                # If extra state information is available:\n",
    "            state = torch.cat([state, batch[\"state\"].cuda()], dim=-1)\n",
    "            target_state = torch.cat([target_state, batch[\"state\"].cuda()], dim=-1)\n",
    "\n",
    "            chosen_action_qvals = self.mixer(chosen_action_qvals, state[:, :-1])\n",
    "            target_max_qvals = self.target_mixer(target_max_qvals, target_state[:, 1:])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "  \n",
    "        # Calculate 1-step Q-Learning targets\n",
    "        targets = rewards_total + self.config[\"gamma\"] * (1 - terminated) * target_max_qvals\n",
    "        \n",
    "        # Td-error\n",
    "        td_error = (chosen_action_qvals - targets.detach())\n",
    "\n",
    "        mask = mask.expand_as(td_error)\n",
    "\n",
    "        # 0-out the targets that came from padded data\n",
    "        masked_td_error = td_error * mask\n",
    "\n",
    "\n",
    "        # Normal L2 loss, take mean over actual data\n",
    "        if self.config[\"curiosity\"]:\n",
    "            qmix_loss = (masked_td_error ** 2).sum() / mask.sum()\n",
    "            forward_loss = self.beta*forward_loss\n",
    "            inverse_loss = (1-self.beta)*inverse_loss\n",
    "            icm_loss = forward_loss + inverse_loss\n",
    "            loss = qmix_loss + icm_loss\n",
    "        else:\n",
    "            loss = (masked_td_error ** 2).sum() / mask.sum()\n",
    "        \n",
    "        # Optimise\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(self.trainable_parameters, self.config[\"grad_norm_clip\"])\n",
    "        self.optimiser.step()\n",
    "\n",
    "        # Update targets:\n",
    "        if (num_global_episodes - self.previous_target_update_episode) / self.config[\"target_update_interval\"] >= 1.0:\n",
    "            self._update_targets()\n",
    "            self.previous_target_update_episode = num_global_episodes\n",
    "        \n",
    "        if log_this_step:\n",
    "            losses_dict = {\"total_loss\" : loss.item()}\n",
    "            if self.config[\"curiosity\"]:\n",
    "                curiosity_stats = {\n",
    "                    \"qmix_loss\" : qmix_loss.item(),\n",
    "                    \"icm_loss\": icm_loss.item(),\n",
    "                    \"forward_loss\" : forward_loss.item(),\n",
    "                    \"inverse_loss\": inverse_loss.item(),\n",
    "                }\n",
    "                losses_dict.update(curiosity_stats)\n",
    "            \n",
    "            mask_elems = mask.sum().item()\n",
    "            other_log_stuff = {\n",
    "                \"grad_norm\": grad_norm,\n",
    "                \"td_error_abs\": (masked_td_error.abs().sum().item()/mask_elems),\n",
    "                \"q_taken_mean\": (chosen_action_qvals * mask).sum().item()/(mask_elems * self.config[\"n_agents\"]),\n",
    "                \"target_mean\": (targets * mask).sum().item()/(mask_elems * self.config[\"n_agents\"])\n",
    "            }\n",
    "            losses_dict.update(other_log_stuff)\n",
    "            return losses_dict\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def update_parameter_server(self):\n",
    "        params = []\n",
    "        state_dicts_to_save = self.mac.agent.state_dict() # returns [agent_state, feature_state]\n",
    "\n",
    "        for param in self.params_list:\n",
    "            params.append(state_dicts_to_save[param].cpu().numpy())\n",
    "        self.parameter_server.update_params.remote(params)\n",
    "\n",
    "    def return_parameter_list(self):\n",
    "        return self.parameter_server_list\n",
    "\n",
    "    def cuda(self):\n",
    "        self.mac.cuda()\n",
    "        self.target_mac.cuda()\n",
    "        if self.mixer is not None:\n",
    "            self.mixer.cuda()\n",
    "            self.target_mixer.cuda()\n",
    "\n",
    "    def _update_targets(self):\n",
    "        self.target_mac.load_state(self.mac)\n",
    "        if self.mixer is not None:\n",
    "            self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "        # self.logger.console_logger.info(\"Updated target network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing ALLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from components.replay_buffer import Remote_ReplayBuffer, generate_replay_scheme\n",
    "from components.parameter_server import ParameterServer\n",
    "\n",
    "ray.init()\n",
    "\n",
    "config = merge_yaml_files(file1, file2)\n",
    "\n",
    "\n",
    "worker = Executor(config, 0)\n",
    "# workers = [Executor.remote(config, i) for i in range (config[\"num_executors\"])]\n",
    "config = worker.retrieve_updated_config()\n",
    "# config = ray.get(config_ref)\n",
    "\n",
    "scheme, groups, preprocess = generate_replay_scheme(config)\n",
    "\n",
    "remote_buffer = Remote_ReplayBuffer(scheme, groups, config[\"buffer_size\"], config[\"episode_limit\"]+1, preprocess=preprocess, device=\"cpu\")\n",
    "parameter_server = ParameterServer()\n",
    "\n",
    "learner = Learner(config)\n",
    "\n",
    "learner.remote_buffer = remote_buffer\n",
    "learner.parameter_server = parameter_server\n",
    "worker.remote_buffer = remote_buffer\n",
    "worker.parameter_server=parameter_server\n",
    "\n",
    "# all_actors = workers + [learner]\n",
    "\n",
    "# # ray.wait([worker.run.remote(remote_buffer, parameter_server) for worker in workers])\n",
    "# ray.wait([worker.run.remote(remote_buffer, parameter_server) for worker in all_actors])\n",
    "\n",
    "episode_batch = worker.collect_experience()\n",
    "\n",
    "remote_buffer.insert_episode_batch(episode_batch)\n",
    "\n",
    "learner.train(log_this_step=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from controllers.custom_controller import CustomMAC\n",
    "from utils.read_config import merge_yaml_files, merge_dicts\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "file1 = \"./config/default.yaml\"\n",
    "file2 = \"./config/visual_qmix.yaml\"\n",
    "\n",
    "\n",
    "config = merge_yaml_files(file1, file2)\n",
    "\n",
    "config[\"obs_shape\"] = (84,84,3)\n",
    "config[\"n_actions\"] = 9\n",
    "mac = CustomMAC(config)\n",
    "\n",
    "sd = mac.agent.state_dict()\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    params = mac.agent.named_parameters().dict\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "# # these params are to be loaded into new_mac\n",
    "# server_parameters = mac.agent.state_dict()\n",
    "\n",
    "# # print(server_parameters.values())\n",
    "\n",
    "# for parameter_name in server_parameters:\n",
    "#     print(server_parameters[parameter_name])\n",
    "#     # print(parameter_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAgents_Portal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

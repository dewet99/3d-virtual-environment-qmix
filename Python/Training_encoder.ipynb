{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.NatureVisualEncoder import NatureVisualEncoder, conv_output_shape, pool_out_shape\n",
    "# from encoders.ResNetVisualEncoder import ResNetVisualEncoder, ResNetBlock, Swish\n",
    "# from src.encoders.NatureVisualAttentionEncoder import NatureVisualAttnEncoder\n",
    "# from utils.encoder_utils import conv_output_shape, pool_out_shape\n",
    "# from utils.rl_utils import RunningMeanStd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from utils.utils import RunningMeanStdTorch as RunningMeanStd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nature Visual Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NatureVisualDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, height: int, width: int, initial_channels: int, output_size: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.h_size = output_size\n",
    "        conv_1_hw = conv_output_shape((height, width), 8, 4)\n",
    "        conv_2_hw = conv_output_shape(conv_1_hw, 4, 2)\n",
    "        conv_3_hw = conv_output_shape(conv_2_hw, 3, 1)\n",
    "        self.final_flat = conv_3_hw[0] * conv_3_hw[1] * 64\n",
    "\n",
    "        # linear = nn.Linear(self.h_size, self.final_flat)\n",
    "        # nn.init.kaiming_normal_(linear.weight.data, nonlinearity=\"linear\")\n",
    "        \n",
    "        # The above is exactly as unity did it, except dumbed down to a single use case\n",
    "        # i.e don't have a bunch of ifs to support different initializers\n",
    "\n",
    "        # self.dense = nn.Sequential(\n",
    "        #     linear,\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "        # self.dense = nn.Sequential(\n",
    "        #     # nn.Tanh(),\n",
    "        #     nn.Linear(self.h_size, self.final_flat),\n",
    "        #     nn.BatchNorm1d(self.final_flat),\n",
    "        #     # nn.ReLU()\n",
    "        # )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(self.h_size, 256),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.final_flat),\n",
    "            # nn.BatchNorm1d(self.final_flat),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # self.dense = nn.Sequential(\n",
    "        #     nn.Linear(self.h_size, self.final_flat),\n",
    "        #     nn.BatchNorm1d(self.final_flat),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            # nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 64, [3, 3], [1, 1]),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, [4, 4], [2, 2]),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, initial_channels, [8, 8], [4, 4]),\n",
    "            # nn.BatchNorm2d(initial_channels),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Sigmoid(),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        # input = self.inverse_z_score_norm(features)\n",
    "        # input = self.inverse_min_max_norm(features, minval, maxval)\n",
    "        hidden = self.dense(features)\n",
    "        hidden = hidden.view([-1, 64, 7, 7])  # reshape to match conv_3_hw\n",
    "        hidden = self.deconv_layers(hidden)\n",
    "        hidden = hidden.permute([0, 2, 3, 1])\n",
    "        hidden = (hidden+1)/2\n",
    "        return hidden\n",
    "    \n",
    "    def inverse_z_score_norm(self, input):\n",
    "        mu = input.mean(dim=-1, keepdim=True)\n",
    "        sigma = input.var(dim=-1, keepdim=True)\n",
    "        \n",
    "        z = (input*sigma)+mu\n",
    "        return z\n",
    "    \n",
    "    def inverse_min_max_norm(self, input, min, max):\n",
    "        # unscaled = (input+1)/2\n",
    "        shifted = input*max\n",
    "        output = shifted+min\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set learning rate\n",
    "lr = 0.0005\n",
    "alpha = 0.99\n",
    "eps = 0.00001\n",
    "\n",
    "# Manual seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "optimizer = torch.optim.Adam\n",
    "# optimizer = torch.optim.RMSprop\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = NatureVisualEncoder(84, 84, 3, 256).cuda()\n",
    "decoder = NatureVisualDecoder(84, 84, 3, 256).cuda()\n",
    "# obs_mean = RunningMeanStd(shape=(2,84,84,3))\n",
    "\n",
    "encoder.set_is_pretraining()\n",
    "try:\n",
    "    # encoder.load_state_dict(torch.load('encoder_NEW.pth'))\n",
    "    # decoder.load_state_dict(torch.load('decoder_NEW.pth'))\n",
    "    pass\n",
    "\n",
    "except:\n",
    "    print(\"No data\")\n",
    "\n",
    "parameters = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "# optimiser = optimizer(parameters, lr, weight_decay=1e-05)\n",
    "optimiser = optimizer(params = parameters, lr=lr, eps=eps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a dataset to validate that the implementations work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# encoder.eval()\n",
    "# decoder.eval()\n",
    "# # Load the image\n",
    "# # test_image = Image.open(\"test_img_2.png\")\n",
    "# # test_image = np.load(\"test_img_2.npy\")\n",
    "# test_image = np.load(\"test_images_1.npy\")[-1]\n",
    "\n",
    "# data0 = Image.fromarray(np.uint8(test_image[:,:,:]*255), mode = \"RGB\")\n",
    "# data0.save(\"input_obs.png\")\n",
    "# display(data0)\n",
    "# # display(test_image)\n",
    "# print(test_image)\n",
    "# # test_image_array = np.array(test_image.getdata())\n",
    "# # print(test_image_array.shape)\n",
    "# # test_image_array = np.array(test_image.getdata()).reshape(test_image.size[0], test_image.size[1], 3)\n",
    "\n",
    "# # Convert it to float from uint8\n",
    "# # test_image_array = test_image_array/255\n",
    "# # test_image_array shape is now (84, 84, 3)\n",
    "\n",
    "# # encode\n",
    "# # encoded_test_img, minval, maxval = encoder(test_image_array)\n",
    "# # print(test_image_array.shape)\n",
    "# encoded_test_img = encoder.forward(test_image)\n",
    "# print(encoded_test_img.shape)\n",
    "\n",
    "# # decode\n",
    "# decoded_test_img = decoder(encoded_test_img)\n",
    "# print(decoded_test_img.shape)\n",
    "\n",
    "\n",
    "# # convert to numpy and remove first dimension\n",
    "# decoded_test_img_ready = decoded_test_img.squeeze(dim=0).cpu().detach().numpy()\n",
    "# print(decoded_test_img_ready.shape)\n",
    "\n",
    "# data0 = Image.fromarray(np.uint8(decoded_test_img_ready[:,:,:]*255), mode = \"RGB\")\n",
    "# display(data0)\n",
    "# data0.save(\"output_obs.png\")\n",
    "\n",
    "# print(encoded_test_img)\n",
    "# print(decoded_test_img)\n",
    "# # print(np.max(decoded_test_img, axis=-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlagents\n",
    "# from mlagents_envs.environment import UnityEnvironment\n",
    "# from mlagents_envs.side_channel.engine_configuration_channel import (EngineConfigurationChannel,)\n",
    "# from wrappers.UnityParallelEnvWrapper_Torch import UnityWrapper\n",
    "# from mlagents_envs.base_env import ActionTuple\n",
    "# import pdb\n",
    "# from collections import deque\n",
    "# import numpy as np\n",
    "# import gc\n",
    "\n",
    "\n",
    "\n",
    "# cwd = os.getcwd()\n",
    "# def get_worker_id(filename=\"worker_id.dat\"):\n",
    "#     with open(filename, 'a+') as f:\n",
    "#         f.seek(0)\n",
    "#         val = int(f.read() or 0) + 1\n",
    "#         f.seek(0)\n",
    "#         f.truncate()\n",
    "#         f.write(str(val))\n",
    "#         return val\n",
    "# config_channel = EngineConfigurationChannel()\n",
    "# config_channel.set_configuration_parameters(time_scale=10.0)\n",
    "\n",
    "\n",
    "# # This is for training in the editor. For training using an executable, setthe file name=path\n",
    "# # env=UnityEnvironment(file_name=None, seed=1, side_channels=[config_channel], worker_id=0)\n",
    "# # env.close()\n",
    "\n",
    "# try:\n",
    "#     env.close()\n",
    "#     unity_env.close()\n",
    "# except:\n",
    "#     print(\"No envs open\")\n",
    "\n",
    "\n",
    "# unity_env = UnityEnvironment(file_name='./environmentExecutables/DiscreteCurriculum/DiscreteCurriculum.x86_64', worker_id=get_worker_id(), seed=np.int32(0), side_channels=[config_channel])\n",
    "# # unity_env = UnityEnvironment(file_name='./unity/envs/Discrete_NoCur/Discrete_NoCur.x86_64', worker_id=get_worker_id())\n",
    "# unity_env.reset()\n",
    "\n",
    "# behaviour_name = list(unity_env.behavior_specs)[0]\n",
    "# behaviour_specs = unity_env.behavior_specs[behaviour_name]\n",
    "\n",
    "# # print(behaviour_specs.observation_specs)\n",
    "# env = unity_env\n",
    "# env.reset()\n",
    "# # decision_steps, _ = env.get_steps(behaviour_name)\n",
    "# # # np.zeros(decision_steps[0].obs[0].shape).astype(np.float32)\n",
    "# # print(decision_steps[0].obs[0].shape)\n",
    "\n",
    "# # nvec = np.zeros(decision_steps[0].obs[0].shape).astype(np.float64)\n",
    "# # nvec = np.zeros(behaviour_specs.action_spec.discrete_size)\n",
    "# # print(nvec)\n",
    "# # empt_act = behaviour_specs.action_spec[1]\n",
    "# # print(empt_act)\n",
    "\n",
    "# decision_steps, terminal_steps = env.get_steps(behaviour_name)\n",
    "\n",
    "# continuous = np.zeros((2,behaviour_specs.action_spec.continuous_size)).astype(np.float32)\n",
    "\n",
    "# episode_length = 300\n",
    "# num_episodes = 25\n",
    "# num_datasets = 12\n",
    "# episodes = deque()\n",
    "# for dataset in range(num_datasets):\n",
    "#     episodes.clear()\n",
    "#     for ep in range(num_episodes):\n",
    "#         # reset the environment\n",
    "#         env.reset()\n",
    "#         episode_observations = deque()\n",
    "#         done = False\n",
    "#         decision_steps, terminal_steps = env.get_steps(behaviour_name)\n",
    "#         while not done:\n",
    "#             # print(f\"Step before reset: {step}\")\n",
    "#             # if reset_next:\n",
    "#             #     env.reset()\n",
    "#             #     reset_next=False\n",
    "#             #     break\n",
    "               \n",
    "\n",
    "#             # Generate a random action and step the environment\n",
    "#             rand1 = np.random.random_integers(0, 6)\n",
    "#             rand2 = np.random.random_integers(0, 6)\n",
    "\n",
    "#             actions = np.array([[rand1], [rand2]])\n",
    "#             # action = ActionTuple(continuous=continuous, discrete=actions)\n",
    "#             action = ActionTuple()\n",
    "#             action.add_discrete(actions)\n",
    "\n",
    "#             env.set_actions(behaviour_name, action)\n",
    "#             env.step()\n",
    "            \n",
    "#             # Get an observation from the environment\n",
    "#             decision_steps, terminal_steps = env.get_steps(behaviour_name)\n",
    "            \n",
    "#             # print(decision_steps.agent_id_to_index)\n",
    "#             if len(terminal_steps)>0:\n",
    "#                 steps_to_use = terminal_steps\n",
    "#                 done =True\n",
    "#                 # reset_next = True\n",
    "                \n",
    "#             else:\n",
    "#                 steps_to_use = decision_steps\n",
    "#                 # reset_next = False\n",
    "                \n",
    "#             episode_observations.append(np.float16(steps_to_use.obs[0]))\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "#         # episode_observations_array = np.array(episode_observations)\n",
    "#         episodes.append(episode_observations)\n",
    "\n",
    "\n",
    "\n",
    "#     # eps = []\n",
    "#     # for ep in episodes:\n",
    "#     #     print(ep.shape)\n",
    "#     #     if ep.shape != (2000, 2, 84, 84, 3):\n",
    "#     #         print(f\"Episode is wrong shape\")\n",
    "#     #         # raise AssertionError\n",
    "#     #         # print(episodes_array[i].shape)\n",
    "#     #     else:\n",
    "#     #         eps.append(ep)\n",
    "\n",
    "#     # episodes_array=np.array(eps)\n",
    "#     file = f\"Datasets/Curriculum_Dataset_{dataset}\"\n",
    "\n",
    "#     np.save(file, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env.close()\n",
    "    unity_env.close()\n",
    "except:\n",
    "    print(\"No envs open\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a few images to use as visual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlagents\n",
    "# from mlagents_envs.environment import UnityEnvironment\n",
    "# from mlagents_envs.side_channel.engine_configuration_channel import (EngineConfigurationChannel,)\n",
    "# from src.wrappers.UnityParallelEnvWrapper_Torch import UnityWrapper\n",
    "# from mlagents_envs.base_env import ActionTuple\n",
    "# import pdb\n",
    "# from collections import deque\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "# def get_worker_id(filename=\"worker_id.dat\"):\n",
    "#     with open(filename, 'a+') as f:\n",
    "#         f.seek(0)\n",
    "#         val = int(f.read() or 0) + 1\n",
    "#         f.seek(0)\n",
    "#         f.truncate()\n",
    "#         f.write(str(val))\n",
    "#         return val\n",
    "# config_channel = EngineConfigurationChannel()\n",
    "# config_channel.set_configuration_parameters(time_scale=100.0)\n",
    "\n",
    "\n",
    "# # This is for training in the editor. For training using an executable, setthe file name=path\n",
    "# # env=UnityEnvironment(file_name=None, seed=1, side_channels=[config_channel], worker_id=0)\n",
    "# # env.close()\n",
    "\n",
    "# try:\n",
    "#     env.close()\n",
    "#     unity_env.close()\n",
    "# except:\n",
    "#     print(\"No envs open\")\n",
    "\n",
    "\n",
    "# unity_env = UnityEnvironment(file_name='./src/environmentExecutables/DiscreteCurriculum/DiscreteCurriculum.x86_64', worker_id=get_worker_id(), seed=np.int32(0), side_channels=[config_channel])\n",
    "# # unity_env = UnityEnvironment(file_name='./unity/envs/Discrete_NoCur/Discrete_NoCur.x86_64', worker_id=get_worker_id())\n",
    "# unity_env.reset()\n",
    "\n",
    "# behaviour_name = list(unity_env.behavior_specs)[0]\n",
    "# behaviour_specs = unity_env.behavior_specs[behaviour_name]\n",
    "\n",
    "# # print(behaviour_specs.observation_specs)\n",
    "# env = unity_env\n",
    "# env.reset()\n",
    "\n",
    "# decision_steps, terminal_steps = env.get_steps(behaviour_name)\n",
    "\n",
    "# continuous = np.zeros((2,behaviour_specs.action_spec.continuous_size)).astype(np.float32)\n",
    "\n",
    "# episode_length = 1500\n",
    "# num_episodes = 1\n",
    "# episodes = deque()\n",
    "# for ep in range(num_episodes):\n",
    "#     # reset the environment\n",
    "#     env.reset()\n",
    "#     for step in range (episode_length):\n",
    "        \n",
    "#         # Get an observation from the environment\n",
    "#         decision_steps, terminal_steps = env.get_steps(behaviour_name)\n",
    "#         if len(terminal_steps)>0:\n",
    "#             steps_to_use = terminal_steps\n",
    "#         else:\n",
    "#             steps_to_use = decision_steps\n",
    "        \n",
    "#         if step%50 ==0:\n",
    "#             observations = steps_to_use.obs[0]\n",
    "#             data0 = Image.fromarray(np.uint8(observations[1,:,:,:]*255), mode = \"RGB\")\n",
    "#             data0.save(f\"./test_imgs/out{np.uint8(step/50)}.png\")\n",
    "            \n",
    "\n",
    "#         # Generate a random action and step the environment\n",
    "#         rand1 = np.random.random_integers(0, 8)\n",
    "#         rand2 = np.random.random_integers(0, 8)\n",
    "\n",
    "#         actions = np.array([[rand1], [rand2]])\n",
    "#         action = ActionTuple(continuous=continuous, discrete=actions)\n",
    "\n",
    "#         env.set_actions(behaviour_name, action)\n",
    "#         env.step()\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset from the saved file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from torchvision.utils import make_grid\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_set(encoder, decoder, device, training_set, loss_fn, optimiser, grayscale):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    train_loss = []\n",
    "\n",
    "    if grayscale:\n",
    "        training_set = convert_to_grayscale(training_set)\n",
    "\n",
    "    # apply the same conversion that we do during RL training, for consistency\n",
    "    training_set = np.float32(np.uint8(training_set*255))/255\n",
    "\n",
    "    batchloader = DataLoader(training_set, batch_size=4096, shuffle=True)\n",
    "    \n",
    "    for image_batch in batchloader:\n",
    "        # image_batch has shape (batch_size, 84, 84, 3)\n",
    "        image_batch = image_batch.to(device)\n",
    "        encoded_data = encoder(image_batch)\n",
    "        decoded_data = decoder(encoded_data)\n",
    "\n",
    "        # print(f\"batch shape:{image_batch.shape}\")\n",
    "        # print(f\"Decoded shape: {decoded_data.shape}\")\n",
    "\n",
    "        # print(f\"batch dtype:{image_batch.dtype}\")\n",
    "        # print(f\"Decoded dtype: {decoded_data.dtype}\")\n",
    "        loss = loss_fn(decoded_data, image_batch.to(torch.float32))\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        print('\\t partial train loss (single batch): %f' % (loss.data))\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "            \n",
    "    # return np.mean(train_loss)\n",
    "    return train_loss\n",
    "\n",
    "            \n",
    "def test_one_set(encoder, decoder, device, testing_set, loss_fn, grayscale):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "\n",
    "    if grayscale:\n",
    "        testing_set = convert_to_grayscale(testing_set)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "\n",
    "        batchloader = DataLoader(testing_set, batch_size=4096, shuffle=True)\n",
    "\n",
    "        for image_batch in batchloader:\n",
    "            # image_batch has shape (batch_size, 84, 84, 3)\n",
    "            image_batch = image_batch.to(device)\n",
    "            encoded_data = encoder(image_batch)\n",
    "            decoded_data = decoder(encoded_data)\n",
    "\n",
    "            conc_out.append(decoded_data.cpu())\n",
    "            conc_label.append(image_batch.cpu())\n",
    "\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label)\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "        print('\\t partial test loss (single set): %f' % (val_loss.data))\n",
    "                \n",
    "    return val_loss.data\n",
    "\n",
    "def save_validation_images(test_images, encoder, decoder, epoch):\n",
    "    \"\"\"\n",
    "    Show input and output images during training stage\n",
    "    \"\"\"\n",
    "    # dataset shape is (-1, 84, 84, 3\n",
    "\n",
    "    encoded_images = encoder(test_images)\n",
    "    decoded_images = decoder(encoded_images)\n",
    "\n",
    "    np.save(f\"encoder_training_results/test_images/decoded_images_{epoch}\", decoded_images.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "def ownmin(x, y):\n",
    "    if x < y:\n",
    "        return x\n",
    "    else:\n",
    "        return y\n",
    "    \n",
    "def convert_to_grayscale(obs):\n",
    "        obs = np.sum(obs*np.array([0.299, 0.587, 0.114]), axis=-1, keepdims=True)\n",
    "        return obs\n",
    "    \n",
    "\n",
    "def process_dataset(dataset_name: str, running_mean, normalise_input_obs =  False, grayscale = False):\n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "    dataset = np.load(dataset_name)\n",
    "\n",
    "    np.load = np_load_old\n",
    "\n",
    "    episodes = []\n",
    "    shape = sum(len(ep) for ep in dataset)\n",
    "\n",
    "    eparr = np.empty((shape,) + (2, 84, 84, 3))\n",
    "\n",
    "    index = 0\n",
    "   \n",
    "    for ep in dataset:\n",
    "        ep_len = len(ep)\n",
    "        eparr[index:index+ep_len] = ep\n",
    "        index += ep_len\n",
    "    \n",
    "    # running_mean.update(eparr)\n",
    "    if normalise_input_obs:\n",
    "        eparr = (eparr - running_mean.mean)/np.sqrt(running_mean.var)\n",
    "\n",
    "\n",
    "    return eparr\n",
    "\n",
    "def generate_means_from_dataset(dataset_name: str, running_mean):\n",
    "    np_load_old = np.load\n",
    "\n",
    "    # modify the default parameters of np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "    dataset = np.load(dataset_name)\n",
    "\n",
    "    np.load = np_load_old\n",
    "\n",
    "    episodes = []\n",
    "    shape = sum(len(ep) for ep in dataset)\n",
    "\n",
    "    eparr = np.empty((shape,) + (2, 84, 84, 3))\n",
    "\n",
    "    index = 0\n",
    "   \n",
    "    for ep in dataset:\n",
    "        ep_len = len(ep)\n",
    "        eparr[index:index+ep_len] = ep\n",
    "        index += ep_len\n",
    "    \n",
    "    running_mean.update(eparr)\n",
    "    \n",
    "    # eparr = (eparr - running_mean.mean)/np.sqrt(running_mean.var)\n",
    "    \n",
    "    # return eparr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "num_training_sets = 10\n",
    "num_testing_sets = 2\n",
    "all_train_losses = []\n",
    "all_test_losses = []\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "obs_rms = RunningMeanStd(shape=(2,84,84,3))\n",
    "normalise_obs = False\n",
    "grayscale = False\n",
    "\n",
    "if grayscale:\n",
    "    shape = (-1,84,84,1)\n",
    "else:\n",
    "    shape = (-1,84,84,3)\n",
    "\n",
    "# test_set_for_thesis = np.load(\"encoder_training_results/test_images/test_image_set.npy\")\n",
    "\n",
    "# Generate the RMS stuff first and do not modify again\n",
    "if normalise_obs:\n",
    "    generate_means_from_dataset(f\"Datasets/Curriculum_Dataset_{0}.npy\", obs_rms)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_losses = []\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Current epoch: {epoch}\")\n",
    "    print(\"============================================================\")\n",
    "    for set_number in range(num_training_sets):\n",
    "        dataset = process_dataset(f\"Datasets/Curriculum_Dataset_{set_number}.npy\", obs_rms, normalise_obs, grayscale=grayscale)\n",
    "        \n",
    "        # dataset shape is (num_eps, 1500, 2, 84, 84, 3), reshape to (-1, 84, 84, 3)\n",
    "        dataset = np.reshape(dataset, shape).astype(np.float32)\n",
    "        print(dataset.shape)\n",
    "        set_loss = train_one_set(encoder, decoder, \"cuda:0\", dataset, loss_fn, optimiser, grayscale)\n",
    "        epoch_train_losses.append(set_loss)\n",
    "    # Get the mean loss for the current epoch\n",
    "    # all_train_losses.append(np.mean(epoch_train_losses))\n",
    "    all_train_losses.append(epoch_train_losses)\n",
    "\n",
    "    epoch_test_losses = []\n",
    "    for test_set_num in range(num_testing_sets):\n",
    "        testing_set = process_dataset(f\"Datasets/testing_set_{test_set_num}.npy\", obs_rms, normalise_obs, grayscale=grayscale)\n",
    "        testing_set = np.reshape(testing_set, shape).astype(np.float32)\n",
    "        # save_validation_images(test_set_for_thesis, encoder, decoder, 5, epoch)\n",
    "        test_set_loss = test_one_set(encoder, decoder, \"cuda:0\", testing_set, loss_fn, grayscale)\n",
    "        epoch_test_losses.append(test_set_loss)\n",
    "    # all_test_losses.append(np.mean(epoch_test_losses))\n",
    "    all_test_losses.append(epoch_test_losses)\n",
    "\n",
    "\n",
    "torch.save(encoder.state_dict(), \"encoder_NEW.pth\")\n",
    "torch.save(decoder.state_dict(), \"decoder_NEW.pth\")\n",
    "np.save(\"all_train_losses\", all_train_losses)\n",
    "np.save(\"all_test_losses\", all_test_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"encoder_NEW.pth\")\n",
    "torch.save(decoder.state_dict(), \"decoder_NEW.pth\")\n",
    "np.save(\"all_train_losses\", all_train_losses)\n",
    "np.save(\"all_test_losses\", all_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses_arr = np.array(all_train_losses)\n",
    "\n",
    "print(train_losses_arr.shape)\n",
    "print(train_losses_arr[0])\n",
    "# 10 epochs of 10 datasets, each dataset with 4 loss values\n",
    "# shape is (epoch, dataset, 4x loss values)\n",
    "train_losses_per_dataset = np.reshape(train_losses_arr, (10, -1)).reshape(-1)\n",
    "\n",
    "print(train_losses_per_dataset.shape)\n",
    "\n",
    "\n",
    "\n",
    "val_losses_arr = np.array(all_test_losses).reshape(-1)\n",
    "\n",
    "print(val_losses_arr.shape)\n",
    "# Create the figure and axis objects\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data\n",
    "# ax.plot(train_losses_per_dataset)\n",
    "ax.plot(val_losses_arr)\n",
    "\n",
    "\n",
    "# Add axis labels and title\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Array Plot')\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(val_losses_arr)\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Array Plot')\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the encoder imput and output after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "for i in range(30):\n",
    "# Load the image\n",
    "    test_image_name = f\"./test_imgs/out{i}.png\"\n",
    "    test_image = Image.open(test_image_name)\n",
    "    test_image_array = np.array(test_image.getdata()).reshape(test_image.size[0], test_image.size[1], 3)\n",
    "\n",
    "    # Convert it to float from uint8\n",
    "    test_image_array = test_image_array/255\n",
    "    # test_image_array shape is now (84, 84, 3)\n",
    "\n",
    "\n",
    "    # encode\n",
    "    encoded_test_img, minval ,maxval = encoder(test_image_array)\n",
    "    print(encoded_test_img.shape)\n",
    "\n",
    "    # decode\n",
    "    decoded_test_img = decoder(encoded_test_img, minval, maxval)\n",
    "    print(decoded_test_img.shape)\n",
    "\n",
    "\n",
    "    # convert to numpy and remove first dimension\n",
    "    decoded_test_img_ready = decoded_test_img.squeeze(dim=0).cpu().detach().numpy()\n",
    "    print(decoded_test_img_ready.shape)\n",
    "\n",
    "    data0 = Image.fromarray(np.uint8(decoded_test_img_ready[:,:,:]*255), mode = \"RGB\")\n",
    "    data0.save(f\"./output_images/out{i}.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Encoder for use with my RL Problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(encoder.state_dict(), \"encoder_NEW.pth\")\n",
    "torch.save(decoder.state_dict(), \"decoder_NEW.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Encoder Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = np.load(\"encoder_training_results/test_images/test_image_set.npy\")\n",
    "\n",
    "# pass the above test set through autoencoder network with the trained weights\n",
    "# save the output after each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = th.tensor(np.concatenate((test, test), axis = 0))\n",
    "grid = make_grid(images.permute(0,3,1,2), nrow = 5, padding = 1)\n",
    "grid = grid.permute(1,2,0)\n",
    "plt.figure(dpi=170)\n",
    "plt.title('Original/Reconstructed')\n",
    "plt.imshow(grid)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from PIL import Image\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "obs = np.load(\"obs_500.npy\")\n",
    "print(obs.shape)\n",
    "obsqz = obs[0, 0, :, :, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obsqz.shape)\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# clear_output(wait=True)\n",
    "\n",
    "#     # Display the current image\n",
    "# plt.imshow(obsqz)\n",
    "obstens = torch.tensor(obsqz)\n",
    "\n",
    "obs_reshape = torch.reshape(obstens, (2,3,84,84))\n",
    "obs_permute = torch.permute(obstens, (0,3,1,2))\n",
    "\n",
    "assert torch.all(obs_reshape == obs_permute)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i, image in enumerate(list(obsqz)):\n",
    "    # Clear the previous plot\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Display the current image\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "\n",
    "    # Show the current image\n",
    "    display(fig)\n",
    "\n",
    "    # Pause for a short duration (adjust as needed, e.g., 0.1 seconds)\n",
    "    # time.sleep(0.1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def print_iteravely(to_print):\n",
    "    for i in range(to_print.shape[0]):\n",
    "        print(np.sum(to_print[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00646253]\n",
      " [0.00646259]\n",
      " [0.00646261]\n",
      " [0.00646262]\n",
      " [0.00646265]\n",
      " [0.00646265]\n",
      " [0.00646266]\n",
      " [0.00646266]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]\n",
      " [0.00646267]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "td_ero = np.load(\"td_er.npy\")\n",
    "mask = np.load(\"mask.npy\")\n",
    "masked_td_error = np.load(\"masked_td_error.npy\")\n",
    "rewards = np.load(\"rewards.npy\")\n",
    "terminated = np.load(\"terminated.npy\")\n",
    "target_max_qvals = np.load(\"target_max_qvals.npy\")\n",
    "td_error = np.load(\"td_error.npy\")\n",
    "chosen_action_qvals = np.load(\"chosen_action_qvals.npy\")\n",
    "targets =np.load(\"targets.npy\")\n",
    "\n",
    "error_index = 16\n",
    "\n",
    "# print(td_ero[error_index])\n",
    "# print(np.sum(masked_td_error**2, axis=(1,2))[error_index]) # the corresponding value is already wrong here\n",
    "# print(np.sum(mask, axis = (1,2))[error_index]) # The corresponding value is already wrong here\n",
    "# print(td_error[error_index]) # Values here seem off, all values are same and is quite large\n",
    "# print(chosen_action_qvals[error_index])# Values here seem off, all values are same and is quite small\n",
    "\n",
    "\n",
    "# td_error is (chosen_action_qvals - targets.detach()) so check both of those\n",
    "# targets comes from the n-step bellman. If the issue is not one of the above, it lies there. Save the values used to calculate bellman and try to replicate \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAgents_Portal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

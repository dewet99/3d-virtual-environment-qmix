action_selector: epsilon_greedy
agent: icm_agent
agent_output_type: q
batch_size: 32
batch_size_run: 1
beta: 0.2
buffer_cpu_only: true
buffer_size: 2400
burn_in_step_count: 32
checkpoint_path: ''
critic_lr: 0.0001
curiosity: false
double_q: true
encoder: nature
encoder_logs_dir: ''
encoder_output_size: 256
env: sc2
env_args: {}
episode_limit: 300
epsilon_anneal_time: 1000000
epsilon_finish: 0.05
epsilon_start: 1.0
eta: 1.0
evaluate: false
executable_path: ./environment_executables/DiscreteCurriculum/DiscreteCurriculum.x86_64
gamma: 0.99
grad_norm_clip: 20
grayscale: false
hypernet_embed: 64
hypernet_layers: 2
icm_weight: 0.003
label: default_label
lamda: 0.1
learner: ray_learner
learner_log_interval: 2000
load_pretrained_model: true
load_step: 0
local_results_path: results
log_every: 20
log_histograms: false
log_interval: 2000
lr: 0.0005
mac: custom_mac
max_instrinsic_reward_sum: 20
min_icm_reward_gain: 1.0e-06
mixer: qmix
mixing_embed_dim: 32
n_actions: 9
n_step: 4
n_step_return: false
name: vis_qmix
normalise_obs: false
normalising_episodes: 10
num_agents: 2
num_executors: 2
num_random_steps: 0
num_training_steps_per_xp: 2
obs_agent_id: true
obs_last_action: true
obs_shape: !!python/tuple
- 84
- 84
- 3
optim_alpha: 0.99
optim_eps: 1.0e-05
pred_lr_scale: 1.0
prioritized_buffer_alpha: 0.6
random_update: true
recurrent_sequence_length: 96
repeat_id: 1
reward_clip_max: 5
reward_clip_min: -5
reward_clipping: true
rnn_hidden_dim: 64
runner: ray_runner
runner_log_interval: 2000
save_model: false
save_model_interval: 2000000
save_models: true
save_models_interval: 500
save_obs_for_debug: false
save_replay: false
standardise_rewards: true
state_shape: 30
t_max: 200000000
target_update_interval: 100
tb_directory: ''
test_greedy: true
test_interval: 15000
test_nepisode: 5
time_scale: 5
useNoisy: false
use_burnin: true
use_cuda: true
use_curiosity_reward: true
use_per: false
use_ray: true
use_tensorboard: true
value_fn_rescaling: true
worker_parameter_sync_frequency: 3
